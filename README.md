# üß† Formspring Data Cyberbullying Classification

## üìò Project Description  
The **Formspring Data Cyberbullying Classification** project aims to detect and classify instances of cyberbullying within online conversations. Using natural language processing (NLP) and deep learning techniques, the model identifies whether a given text contains bullying, aggression, or offensive content. This project helps promote safer digital communication environments by automatically flagging harmful messages.

---

## üóÇÔ∏è Project Agenda
#### 1Ô∏è‚É£ üìä Dataset Description  
#### 2Ô∏è‚É£ üè∑Ô∏è Data Labeling  
#### 3Ô∏è‚É£ üßπ Data Pre-processing  
#### 4Ô∏è‚É£ üß† Model Training  
#### 5Ô∏è‚É£ üìà Results  
#### 6Ô∏è‚É£ üíª Sample Outputs from Streamlit Deployment  


---

 
## 1Ô∏è‚É£ üìä Dataset Description: Formspring Cyberbullying Detection (Reynolds et al., 2011)

#### üßæ Overview  
The **Formspring Cyberbullying Detection Dataset** is a benchmark dataset designed for research on online harassment and abusive language detection. It was collected from the now-defunct **Formspring.me** Q&A social platform during the **summer of 2010**.  
The dataset includes **12,772 anonymous question-answer pairs**, each annotated by **three independent workers on Amazon Mechanical Turk** to determine whether the post contains cyberbullying content.

| Statistic | Value |
|------------|--------|
| **Total Samples** | 12,901 |
| **Total Tokens** | ~300,000 |
| **Average Words per Post** | 23 (bullying) / 25 (non-bullying) |

> ‚ö†Ô∏è **Note:** The dataset is highly imbalanced, with over **80% non-bullying samples**, reflecting the natural distribution of online conversations.

#### üìÇ Data Fields  
| Field | Description |
|--------|-------------|
| `Userid` | Identifier of the respondent |
| `Asker` | Identifier of the question asker |
| `Post` | Combined text of question and answer (markers removed during preprocessing) |
| `Ans#` | Annotator binary response (*Yes/No*) for bullying ‚Äì three responses per post |
| `severity#` | Bullying intensity score ranging from **0‚Äì10** |
| `bully#` | Words or phrases flagged as bullying indicators |

#### üß† Annotation Process  
- A post is labeled as **bullying** if **at least one of the three annotators** agreed.
> üí° **Tip:** If you‚Äôd like to explore the dataset before diving into the notebooks, click the button to view the raw data. [![View Dataset](https://img.shields.io/badge/View_Dataset-CSV-blue?style=for-the-badge&logo=github)](https://github.com/Yousef-Jaber-Abdelaziz/Formspring-Data-Cyberbullying-Classification/blob/6d8041e46163d643e09f2fe1ac7cf856d067ce6d/Datasets/0-Root%20Data/formspring.csv)


---

## 2Ô∏è‚É£ üè∑Ô∏è Data Labeling  

Each post in the dataset was annotated by three independent human evaluators, providing binary responses (*Yes/No*) and a **severity score** ranging from 0 to 10.  

To ensure a clear distinction between bullying and non-bullying content, the following custom labeling rule was applied:

```text
IF any annotator assigned a severity > 0:
    ‚Üí Label = 1 (Cyberbullying)
ELSE:
    ‚Üí Label = 0 (Non-bullying)
```
> üßÆ **Labeling Summary:**  
> This logic resulted in **1,915 samples** being annotated as **cyberbullying** based on the applied severity rule.  

> üß≠ **Quick Navigation:**  
> Navigate the notebook [![Jupyter](https://img.shields.io/badge/-Notebook-FFA500?style=flat-square&logo=jupyter&logoColor=white)](https://github.com/Yousef-Jaber-Abdelaziz/Formspring-Data-Cyberbullying-Classification/blob/44fca2ce81215e597b8495defec4c2903643bd45/Project%20Code/Notebooks/0_Cyber-bullying-Class-Split.ipynb)  
> View the splitted data samples [![Dataset](https://img.shields.io/badge/-Datasets-4CAF50?style=flat-square&logo=files&logoColor=white)](https://github.com/Yousef-Jaber-Abdelaziz/Formspring-Data-Cyberbullying-Classification/tree/bcebbe1dbb37099a674dc9a3e59d648a32ff2f0b/Datasets/1-Data%20Class%20Splitting)


---


## 2Ô∏è‚É£ üßπ Data Pre-processing  

All preprocessing steps were implemented in the notebook **`1_Cyber-bullying-Preprocessing.ipynb`** to ensure clean, standardized text input for model training.  

| Step | Description |
|------|-------------|
| **1. Data Loading & Merging** | Imported `bully_data.csv` (label=1) and `not_bully_data.csv` (label=0), then combined and shuffled the data for balanced sampling. |
| **2. HTML, URL & Symbol Cleaning** | Applied `BeautifulSoup` and regular expressions to strip HTML tags, remove URLs, and retain only alphabetic characters and spaces. |
| **3. Slang & Contraction Expansion** | Replaced slang terms (e.g., `"lol"`) and contractions (e.g., `"don‚Äôt"`) using custom JSON dictionaries: `slangs.json` and `contractions.json`. |
| **4. Grammar & Spelling Correction (LLM-based)** | Used **Microsoft Phi-2** model for contextual text correction via the prompt: <br> `"Correct grammar and spelling: {text}"` <br> Truncated long inputs (>500 chars) and extracted corrected text. |
| **5. Punctuation Removal** | Removed punctuation marks using `str.maketrans` to ensure uniform tokenization. |
| **6. Named Entity Replacement** | Leveraged **spaCy** to anonymize named entities, e.g., `"John in New York"` ‚Üí `"person in location"`. |
| **7. Tokenization, Lemmatization & Stopword Removal** | Used **NLTK** to tokenize, POS-tag, and lemmatize text; removed standard English stopwords while preserving critical bullying terms such as `ass`, `bitch`, `fuck`, etc. |
| **8. Final Cleaning** | Dropped empty or NaN entries and removed duplicate posts to ensure data consistency. |
| **9. Export Processed Data** | Saved the final cleaned dataset as `all_data_processed.csv` containing: <br>‚Ä¢ `post`: tokenized text <br>‚Ä¢ `label`: 0 (non-bully) / 1 (bully). |

üìì You can explore the preprocessing notebook here:  
[![Notebook](https://img.shields.io/badge/Jupyter-Notebook-DAA520?style=flat-square&logo=jupyter&logoColor=white)](Project Code/Notebooks/1-cyber-bullying-preprocessing.ipynb)  
üìÇ Or view the processed dataset here:  
[![Dataset](https://img.shields.io/badge/Dataset-Processed_Data-4682B4?style=flat-square&logo=files&logoColor=white)](datasets/processed)
